{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashwin-Rajesh/DeepLearning/blob/master/Nerual-Networks-And-Deep-Learning/NN1-LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pesG3cP_-8je",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression\n",
        "---\n",
        "- Logistic regression is regression applied to binary classification problems.\n",
        "- Example - Does image contain a car?\n",
        "---\n",
        "## Structure and notation\n",
        "- Input is in the form of n values, represented by an **n-dimensional vector**\n",
        "- Output  - A single value, which is **either 0 or 1**\n",
        "---\n",
        "### Equation\n",
        "- The equation used in logistic regression is \n",
        "\n",
        "```\n",
        "# x = inputs\n",
        "# w = weights\n",
        "# b = biases\n",
        "\n",
        "# Here, sigmoid function is used. There are alternatives (RELU,leaky RELU,..)\n",
        "def sigma(z):\n",
        "    return 1 / ( 1 + exp(-z) )\n",
        "\n",
        "z = x.dot(w.T) + b\n",
        "a = sigma(z)\n",
        "```\n",
        "---\n",
        "### Loss and Cost\n",
        "- Loss functions is a function that is used to measure the deviation of predictions from ground truth.\n",
        "- Cost function is average of loss function for all samples in the training set\n",
        "\n",
        "```\n",
        "def Loss(a,y):\n",
        "    return -(y * log(a) + (1 - y) * log(1 - a))\n",
        "```\n",
        "---\n",
        "### Gradient descent\n",
        "- The algorithm that changes w and b to reduce cost.\n",
        "\n",
        "```\n",
        "# dw - dJ/dw\n",
        "# db - dJ/db\n",
        "# Alpha - Learning rate (a hyperparameter)\n",
        "\n",
        "w -= Alpha * dw\n",
        "b -= Alpha * db\n",
        "```\n",
        "---\n",
        "### Calculus\n",
        "- We can find dw and db quite easily.\n",
        "\n",
        "```\n",
        "- L        = -y log(a) - (1 - y) log(1-a)\n",
        "- dL/da    = (1-y)/(1-a) - y/a\n",
        "\n",
        "- a        = 1 / 1 + e^(-z)\n",
        "- da/dz    = a * (1 - a)\n",
        "\n",
        "- dL/dz    = a - y\n",
        "\n",
        "- z        = x.wt + b\n",
        "- dz/dw    = x\n",
        "- dw/db    = b\n",
        "\n",
        "- dL/dw    = x * (a - y)\n",
        "- dL/db    = a - y\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5BKNp-c868E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing required libraries\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}